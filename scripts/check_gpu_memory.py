#!/usr/bin/env python3
"""
GPU æ˜¾å­˜æ£€æŸ¥è„šæœ¬
ç”¨äºæ£€æŸ¥å„ä¸ª GPU çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œå¸®åŠ©é€‰æ‹©åˆé€‚çš„ GPU è®¾å¤‡
"""

import torch
import subprocess
import sys
from typing import List, Dict, Tuple


def get_gpu_memory_info() -> List[Dict]:
    """è·å–æ‰€æœ‰GPUçš„æ˜¾å­˜ä¿¡æ¯"""
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨")
        return []
    
    gpu_count = torch.cuda.device_count()
    gpu_info = []
    
    for i in range(gpu_count):
        try:
            # è·å–GPUå±æ€§
            props = torch.cuda.get_device_properties(i)
            
            # è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            torch.cuda.set_device(i)
            memory_allocated = torch.cuda.memory_allocated(i)
            memory_reserved = torch.cuda.memory_reserved(i)
            memory_total = props.total_memory
            memory_free = memory_total - memory_reserved
            
            gpu_info.append({
                'id': i,
                'name': props.name,
                'total_memory': memory_total,
                'allocated_memory': memory_allocated,
                'reserved_memory': memory_reserved,
                'free_memory': memory_free,
                'utilization_percent': (memory_reserved / memory_total) * 100
            })
            
        except Exception as e:
            print(f"âŒ è·å– GPU {i} ä¿¡æ¯å¤±è´¥: {e}")
            
    return gpu_info


def get_nvidia_smi_info() -> str:
    """è·å– nvidia-smi è¾“å‡º"""
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        return result.stdout
    except Exception as e:
        return f"âŒ æ— æ³•è¿è¡Œ nvidia-smi: {e}"


def format_memory(bytes_value: int) -> str:
    """æ ¼å¼åŒ–å†…å­˜å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.1f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.1f} TB"


def recommend_gpu(gpu_info: List[Dict]) -> int:
    """æ¨èæœ€é€‚åˆçš„GPU"""
    if not gpu_info:
        return -1
    
    # é€‰æ‹©æ˜¾å­˜ä½¿ç”¨ç‡æœ€ä½çš„GPU
    best_gpu = min(gpu_info, key=lambda x: x['utilization_percent'])
    return best_gpu['id']


def main():
    print("ğŸ” GPU æ˜¾å­˜æ£€æŸ¥å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ï¼š")
        print("   1. NVIDIA é©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("   2. PyTorch æ˜¯å¦æ”¯æŒ CUDA")
        print("   3. CUDA ç‰ˆæœ¬æ˜¯å¦å…¼å®¹")
        sys.exit(1)
    
    # è·å–GPUä¿¡æ¯
    gpu_info = get_gpu_memory_info()
    
    if not gpu_info:
        print("âŒ æ— æ³•è·å– GPU ä¿¡æ¯")
        sys.exit(1)
    
    print(f"âœ… æ£€æµ‹åˆ° {len(gpu_info)} ä¸ª GPU è®¾å¤‡")
    print()
    
    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    print("ğŸ“Š GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ:")
    print("-" * 60)
    
    for gpu in gpu_info:
        status_icon = "ğŸŸ¢" if gpu['utilization_percent'] < 10 else "ğŸŸ¡" if gpu['utilization_percent'] < 50 else "ğŸ”´"
        
        print(f"{status_icon} GPU {gpu['id']}: {gpu['name']}")
        print(f"   æ€»æ˜¾å­˜:   {format_memory(gpu['total_memory'])}")
        print(f"   å·²åˆ†é…:   {format_memory(gpu['allocated_memory'])}")
        print(f"   å·²ä¿ç•™:   {format_memory(gpu['reserved_memory'])}")
        print(f"   å¯ç”¨:     {format_memory(gpu['free_memory'])}")
        print(f"   ä½¿ç”¨ç‡:   {gpu['utilization_percent']:.1f}%")
        print()
    
    # æ¨èGPU
    recommended_gpu = recommend_gpu(gpu_info)
    if recommended_gpu >= 0:
        print(f"ğŸ’¡ æ¨èä½¿ç”¨ GPU {recommended_gpu} (æ˜¾å­˜ä½¿ç”¨ç‡æœ€ä½)")
        print(f"   ç¯å¢ƒå˜é‡è®¾ç½®: export SENSEVOICE_DEVICE=cuda:{recommended_gpu}")
        print()
    
    # æ˜¾ç¤ºnvidia-smiè¾“å‡º
    print("ğŸ“‹ nvidia-smi è¾“å‡º:")
    print("-" * 60)
    nvidia_smi_output = get_nvidia_smi_info()
    print(nvidia_smi_output)
    
    # æä¾›ä½¿ç”¨å»ºè®®
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("-" * 60)
    
    free_gpus = [gpu for gpu in gpu_info if gpu['utilization_percent'] < 10]
    if free_gpus:
        print("âœ… å¯ç”¨çš„ç©ºé—² GPU:")
        for gpu in free_gpus:
            print(f"   - GPU {gpu['id']}: {gpu['name']} (ä½¿ç”¨ç‡: {gpu['utilization_percent']:.1f}%)")
        print()
        print("ğŸš€ å¯åŠ¨å‘½ä»¤ç¤ºä¾‹:")
        best_gpu = free_gpus[0]['id']
        print(f"   export SENSEVOICE_DEVICE=cuda:{best_gpu}")
        print(f"   python main.py")
    else:
        print("âš ï¸  æ‰€æœ‰ GPU éƒ½æœ‰è¾ƒé«˜çš„æ˜¾å­˜ä½¿ç”¨ç‡")
        print("   å»ºè®®:")
        print("   1. é‡Šæ”¾å…¶ä»–è¿›ç¨‹å ç”¨çš„æ˜¾å­˜")
        print("   2. ä½¿ç”¨æ˜¾å­˜ä½¿ç”¨ç‡æœ€ä½çš„ GPU")
        print("   3. è€ƒè™‘ä½¿ç”¨ CPU æ¨¡å¼: export SENSEVOICE_DEVICE=cpu")
    
    print()
    print("ğŸ”§ æ˜¾å­˜ç®¡ç†å‘½ä»¤:")
    print("   æ¸…ç†æ˜¾å­˜ç¼“å­˜: python -c \"import torch; torch.cuda.empty_cache()\"")
    print("   æŸ¥çœ‹è¿›ç¨‹: nvidia-smi")
    print("   æ€æ­»è¿›ç¨‹: kill -9 <PID>")


if __name__ == "__main__":
    main()
