#!/usr/bin/env python3
"""
GPU æ˜¾å­˜æ£€æŸ¥è„šæœ¬
ç”¨äºæ£€æŸ¥å„ä¸ª GPU çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œå¸®åŠ©é€‰æ‹©åˆé€‚çš„ GPU è®¾å¤‡
"""

import subprocess
import sys
from typing import List, Dict, Tuple, Optional


def get_gpu_memory_info() -> List[Dict]:
    """è·å–æ‰€æœ‰GPUçš„æ˜¾å­˜ä¿¡æ¯ï¼ˆä½¿ç”¨ pynvml è·å–å‡†ç¡®ä¿¡æ¯ï¼‰"""
    # ä¼˜å…ˆä½¿ç”¨ pynvmlï¼Œå®ƒå¯ä»¥çœ‹åˆ°æ‰€æœ‰è¿›ç¨‹çš„æ˜¾å­˜ä½¿ç”¨
    try:
        import pynvml
        pynvml.nvmlInit()
        
        device_count = pynvml.nvmlDeviceGetCount()
        gpu_info = []
        
        for i in range(device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                
                # è·å–æ˜¾å­˜ä¿¡æ¯
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                # è·å–GPUåç§°
                gpu_name = pynvml.nvmlDeviceGetName(handle)
                if isinstance(gpu_name, bytes):
                    gpu_name = gpu_name.decode('utf-8')
                
                # è·å–ä½¿ç”¨è¯¥GPUçš„è¿›ç¨‹ä¿¡æ¯
                try:
                    processes = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
                    process_info = []
                    for proc in processes:
                        process_info.append({
                            'pid': proc.pid,
                            'memory': proc.usedGpuMemory
                        })
                except:
                    process_info = []
                
                gpu_info.append({
                    'id': i,
                    'name': gpu_name,
                    'total_memory': memory_info.total,
                    'used_memory': memory_info.used,
                    'free_memory': memory_info.free,
                    'utilization_percent': (memory_info.used / memory_info.total) * 100,
                    'processes': process_info
                })
                
            except Exception as e:
                print(f"âŒ è·å– GPU {i} ä¿¡æ¯å¤±è´¥: {e}")
        
        pynvml.nvmlShutdown()
        return gpu_info
        
    except ImportError:
        print("âš ï¸  pynvml åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨ PyTorch APIï¼ˆä»…èƒ½çœ‹åˆ°å½“å‰è¿›ç¨‹ï¼‰")
        return get_gpu_memory_info_pytorch()
    except Exception as e:
        print(f"âš ï¸  ä½¿ç”¨ pynvml å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨ PyTorch API")
        return get_gpu_memory_info_pytorch()


def get_gpu_memory_info_pytorch() -> List[Dict]:
    """ä½¿ç”¨ PyTorch API è·å– GPU ä¿¡æ¯ï¼ˆä»…èƒ½çœ‹åˆ°å½“å‰è¿›ç¨‹ï¼‰"""
    try:
        import torch
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return []
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨")
        return []
    
    gpu_count = torch.cuda.device_count()
    gpu_info = []
    
    for i in range(gpu_count):
        try:
            # è·å–GPUå±æ€§
            props = torch.cuda.get_device_properties(i)
            
            # è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            torch.cuda.set_device(i)
            memory_allocated = torch.cuda.memory_allocated(i)
            memory_reserved = torch.cuda.memory_reserved(i)
            memory_total = props.total_memory
            memory_free = memory_total - memory_reserved
            
            gpu_info.append({
                'id': i,
                'name': props.name,
                'total_memory': memory_total,
                'used_memory': memory_reserved,
                'free_memory': memory_free,
                'utilization_percent': (memory_reserved / memory_total) * 100,
                'processes': [],  # PyTorch API æ— æ³•è·å–è¿›ç¨‹ä¿¡æ¯
                'pytorch_only': True
            })
            
        except Exception as e:
            print(f"âŒ è·å– GPU {i} ä¿¡æ¯å¤±è´¥: {e}")
            
    return gpu_info


def get_nvidia_smi_info() -> str:
    """è·å– nvidia-smi è¾“å‡º"""
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        return result.stdout
    except Exception as e:
        return f"âŒ æ— æ³•è¿è¡Œ nvidia-smi: {e}"


def format_memory(bytes_value: int) -> str:
    """æ ¼å¼åŒ–å†…å­˜å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.1f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.1f} TB"


def recommend_gpu(gpu_info: List[Dict]) -> int:
    """æ¨èæœ€é€‚åˆçš„GPU"""
    if not gpu_info:
        return -1
    
    # é€‰æ‹©æ˜¾å­˜ä½¿ç”¨ç‡æœ€ä½çš„GPU
    best_gpu = min(gpu_info, key=lambda x: x['utilization_percent'])
    return best_gpu['id']


def main():
    print("ğŸ” GPU æ˜¾å­˜æ£€æŸ¥å·¥å…·")
    print("=" * 80)
    
    # è·å–GPUä¿¡æ¯
    gpu_info = get_gpu_memory_info()
    
    if not gpu_info:
        print("âŒ æ— æ³•è·å– GPU ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ï¼š")
        print("   1. NVIDIA é©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("   2. PyTorch æ˜¯å¦æ”¯æŒ CUDA")
        print("   3. CUDA ç‰ˆæœ¬æ˜¯å¦å…¼å®¹")
        print("\nğŸ’¡ æç¤ºï¼š")
        print("   - å®‰è£… pynvml è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯: pip install pynvml")
        print("   - æ£€æŸ¥ nvidia-smi æ˜¯å¦å¯ç”¨: nvidia-smi")
        sys.exit(1)
    
    print(f"âœ… æ£€æµ‹åˆ° {len(gpu_info)} ä¸ª GPU è®¾å¤‡")
    print()
    
    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    print("ğŸ“Š GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ:")
    print("-" * 80)
    
    for gpu in gpu_info:
        status_icon = "ğŸŸ¢" if gpu['utilization_percent'] < 10 else "ğŸŸ¡" if gpu['utilization_percent'] < 50 else "ğŸ”´"
        
        print(f"{status_icon} GPU {gpu['id']}: {gpu['name']}")
        print(f"   æ€»æ˜¾å­˜:   {format_memory(gpu['total_memory'])}")
        print(f"   å·²ä½¿ç”¨:   {format_memory(gpu['used_memory'])}")
        print(f"   å¯ç”¨:     {format_memory(gpu['free_memory'])}")
        print(f"   ä½¿ç”¨ç‡:   {gpu['utilization_percent']:.1f}%")
        
        # æ˜¾ç¤ºè¿›ç¨‹ä¿¡æ¯
        if gpu.get('processes') and len(gpu['processes']) > 0:
            print(f"   å ç”¨è¿›ç¨‹: {len(gpu['processes'])} ä¸ª")
            for proc in gpu['processes'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªè¿›ç¨‹
                print(f"      - PID {proc['pid']}: {format_memory(proc['memory'])}")
            if len(gpu['processes']) > 5:
                print(f"      ... è¿˜æœ‰ {len(gpu['processes']) - 5} ä¸ªè¿›ç¨‹")
        elif gpu.get('pytorch_only'):
            print(f"   âš ï¸  ä»…æ˜¾ç¤ºå½“å‰è¿›ç¨‹ä½¿ç”¨æƒ…å†µï¼ˆéœ€è¦å®‰è£… pynvml æŸ¥çœ‹æ‰€æœ‰è¿›ç¨‹ï¼‰")
        else:
            print(f"   å ç”¨è¿›ç¨‹: 0 ä¸ª")
        print()
    
    # æ¨èGPU
    recommended_gpu = recommend_gpu(gpu_info)
    if recommended_gpu >= 0:
        print(f"ğŸ’¡ æ¨èä½¿ç”¨ GPU {recommended_gpu} (æ˜¾å­˜ä½¿ç”¨ç‡æœ€ä½)")
        print(f"   ç¯å¢ƒå˜é‡è®¾ç½®: export SENSEVOICE_DEVICE=cuda:{recommended_gpu}")
        print()
    
    # æ˜¾ç¤ºnvidia-smiè¾“å‡º
    print("ğŸ“‹ nvidia-smi è¾“å‡º:")
    print("-" * 80)
    nvidia_smi_output = get_nvidia_smi_info()
    print(nvidia_smi_output)
    
    # æä¾›ä½¿ç”¨å»ºè®®
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("-" * 80)
    
    free_gpus = [gpu for gpu in gpu_info if gpu['utilization_percent'] < 10]
    if free_gpus:
        print("âœ… å¯ç”¨çš„ç©ºé—² GPU:")
        for gpu in free_gpus:
            print(f"   - GPU {gpu['id']}: {gpu['name']} (ä½¿ç”¨ç‡: {gpu['utilization_percent']:.1f}%)")
        print()
        print("ğŸš€ å¯åŠ¨å‘½ä»¤ç¤ºä¾‹:")
        best_gpu = free_gpus[0]['id']
        print(f"   export SENSEVOICE_DEVICE=cuda:{best_gpu}")
        print(f"   python main.py")
    else:
        print("âš ï¸  æ‰€æœ‰ GPU éƒ½æœ‰è¾ƒé«˜çš„æ˜¾å­˜ä½¿ç”¨ç‡")
        print("   å»ºè®®:")
        print("   1. é‡Šæ”¾å…¶ä»–è¿›ç¨‹å ç”¨çš„æ˜¾å­˜")
        print("   2. ä½¿ç”¨æ˜¾å­˜ä½¿ç”¨ç‡æœ€ä½çš„ GPU")
        print("   3. è€ƒè™‘ä½¿ç”¨ CPU æ¨¡å¼: export SENSEVOICE_DEVICE=cpu")
    
    print()
    print("ğŸ”§ æ˜¾å­˜ç®¡ç†å‘½ä»¤:")
    print("   å®‰è£…æ˜¾å­˜ç›‘æ§åº“: pip install pynvml")
    print("   æ¸…ç†æ˜¾å­˜ç¼“å­˜: python -c \"import torch; torch.cuda.empty_cache()\"")
    print("   æŸ¥çœ‹è¿›ç¨‹: nvidia-smi")
    print("   æ€æ­»è¿›ç¨‹: kill -9 <PID>")
    print()
    print("ğŸ“– SenseVoice è‡ªåŠ¨ GPU é€‰æ‹©:")
    print("   - é»˜è®¤ä¼šè‡ªåŠ¨é€‰æ‹©æ˜¾å­˜ç©ºé—²æœ€å¤šçš„ GPU")
    print("   - æ‰‹åŠ¨æŒ‡å®š: export SENSEVOICE_DEVICE=cuda:N")
    print("   - ä½¿ç”¨ CPU: export SENSEVOICE_DEVICE=cpu")


if __name__ == "__main__":
    main()
